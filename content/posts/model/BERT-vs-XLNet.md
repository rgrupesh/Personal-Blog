---
title: BERT vs. XLNet
cover: ./language.jpg
date: 2020-08-10
tags: [post,article,Deep Learning, NLP, Transformers, XLNet, BERT, Transfer Learning]
---

Transformer based model has been key to recent advencement in the field of Natural Language Processing. The reason behind this success is technique called Transfer Learning. Although vision practitioners are well-versed with this technique, it is relatively new to the field of NLP. In Transfer Learning, a model (in our case, a Transformer model) is pre-trained on a huge dataset using an unsupervised pre-training objective. This same model is then fine-tuned on the actual task at hand. This approach works exceptionally well, even if you have as small as 500â€“1000 training samples.

<h2>BERT</h2>

<h2>XLNet</h2>

<h2>Which one should you choose?</h2>

![](/scores.png/)

![](/crop.png/)



